{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is part of [Chapter 3: Cats versus Dogs: Transfer Learning in 30 Lines with Keras](https://learning.oreilly.com/library/view/practical-deep-learning/9781492034858/ch03.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Custom Classifier in Keras with Transfer Learning\n",
    "\n",
    "As promised, it’s time to build our state of the art classifier in 30 lines or fewer! At a high level, we will follow the steps shown below:\n",
    "\n",
    "- **Organize the data**: Download labeled images of cats and dogs from Kaggle. Then divide the images into training and validation folders.\n",
    "- **Set up the configuration**: Define a pipeline for reading data, including preprocessing the images (e.g. resizing) and batching multiple images together.\n",
    "- **Load and augment the data**: In the absence of a ton of training images, make small changes (augmentation) like rotation, zooming, etc to increase variation in training data.\n",
    "- **Define the model**: Take a pre-trained model, remove the last few layers, and append a new classifier layer. Freeze the weights of original layers (i.e. make them unmodifiable). Select an optimizer algorithm and a metric to track (like accuracy).\n",
    "- **Train and test**: Start training for a few iterations. Save the model to eventually load inside any application for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organize the data\n",
    "\n",
    "Before training, we need to store our [downloaded dataset](https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/download/train.zip) in the right folder structure. Remember to make the `data` directory where we will be performing the refactoring. We’ll divide the images into two sets – training and validation. Our directory structure will look something like this: \n",
    "\n",
    "```\n",
    "data\n",
    " |__train\n",
    " |    |__cat\n",
    " |    |__dog\n",
    " |__val\n",
    "      |__cat\n",
    "      |__dog\n",
    "```\n",
    "\n",
    "In Windows, the following lines of command can help achieve this directory structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файлы распакованы в папку 'train'.\n",
      "Директории train и val успешно созданы.\n",
      "Файлы успешно распределены по папкам train и val.\n",
      "Путь к директории train: c:\\Users\\Валерия\\Documents\\GitHub\\Practical-Deep-Learning-Book\\code\\chapter-3\\data\\train\n",
      "Путь к директории val: c:\\Users\\Валерия\\Documents\\GitHub\\Practical-Deep-Learning-Book\\code\\chapter-3\\data\\val\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Определение относительного пути для файла train.zip\n",
    "zip_file_path = os.path.join('..', '..', 'Downloads', 'dogs-vs-cats-redux-kernels-edition', 'train.zip')\n",
    "\n",
    "# Определение директории для распаковки\n",
    "data_dir = 'data'  # Основная директория для распаковки и работы\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_dir)  # Распаковываем сразу в 'data'\n",
    "\n",
    "# Проверяем, что файлы были распакованы в data/train\n",
    "train_files_dir = os.path.join(data_dir, 'train')\n",
    "if not os.path.exists(train_files_dir):\n",
    "    print(\"Ошибка: папка 'train' не была создана.\")\n",
    "else:\n",
    "    print(\"Файлы распакованы в папку 'train'.\")\n",
    "\n",
    "# Создание необходимых директорий\n",
    "os.makedirs(os.path.join(data_dir, 'train', 'cat'), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'train', 'dog'), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'val', 'cat'), exist_ok=True)\n",
    "os.makedirs(os.path.join(data_dir, 'val', 'dog'), exist_ok=True)\n",
    "\n",
    "print(\"Директории train и val успешно созданы.\")\n",
    "\n",
    "# Создайте список всех файлов в директории train (там где были распакованы файлы)\n",
    "all_files = os.listdir(train_files_dir)\n",
    "\n",
    "# Фильтруем файлы, относящиеся к котам и собакам\n",
    "cat_files = [f for f in all_files if 'cat' in f and f.endswith('.jpg')]\n",
    "dog_files = [f for f in all_files if 'dog' in f and f.endswith('.jpg')]\n",
    "\n",
    "# Перемешиваем списки случайным образом\n",
    "random.shuffle(cat_files)\n",
    "random.shuffle(dog_files)\n",
    "\n",
    "# Выбираем 250 файлов для каждой категории и перемещаем их в папки train\n",
    "for f in cat_files[:250]:\n",
    "    shutil.move(os.path.join(train_files_dir, f), os.path.join(data_dir, 'train', 'cat', f))\n",
    "for f in dog_files[:250]:\n",
    "    shutil.move(os.path.join(train_files_dir, f), os.path.join(data_dir, 'train', 'dog', f))\n",
    "\n",
    "# Оставшиеся файлы используем для папок val (validation)\n",
    "for f in cat_files[250:500]:\n",
    "    shutil.move(os.path.join(train_files_dir, f), os.path.join(data_dir, 'val', 'cat', f))\n",
    "for f in dog_files[250:500]:\n",
    "    shutil.move(os.path.join(train_files_dir, f), os.path.join(data_dir, 'val', 'dog', f))\n",
    "\n",
    "print(\"Файлы успешно распределены по папкам train и val.\")\n",
    "\n",
    "# Вывод абсолютных путей к папкам train и val\n",
    "train_dir_path = os.path.abspath(os.path.join(data_dir, 'train'))\n",
    "val_dir_path = os.path.abspath(os.path.join(data_dir, 'val'))\n",
    "\n",
    "print(f\"Путь к директории train: {train_dir_path}\")\n",
    "print(f\"Путь к директории val: {val_dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images in train/cat: 250\n",
      "Images in train/dog: 250\n",
      "Images in val/cat: 250\n",
      "Images in val/dog: 250\n"
     ]
    }
   ],
   "source": [
    "print(f\"Images in train/cat: {len(os.listdir(os.path.join(data_dir, 'train', 'cat')))}\")\n",
    "print(f\"Images in train/dog: {len(os.listdir(os.path.join(data_dir, 'train', 'dog')))}\")\n",
    "print(f\"Images in val/cat: {len(os.listdir(os.path.join(data_dir, 'val', 'cat')))}\")\n",
    "print(f\"Images in val/dog: {len(os.listdir(os.path.join(data_dir, 'val', 'dog')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 25,000 files inside the data folder are prefixed with ‘cat’ and ‘dog’. Now, move the files into their respective directories. To keep our initial experiment short, we’ll pick 250 random files per class and place them in training and validation folders. You can increase/decrease this number anytime, to experiment with a trade-off between accuracy and speed. \n",
    "\n",
    "Classification accuracy on previously unseen images (in the validation folder) is a good proxy for how the classifier would perform in the real world. Ideally, the more training images, the better the learning will be. And, the more validation images, the better our classifier would perform in the real-world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the configuration\n",
    "\n",
    "Let's start off with our Python program and begin with importing the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's place all the configurations up-front. These can be modified in the future based on the dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = os.path.join(data_dir, 'train')\n",
    "VALIDATION_DATA_DIR = os.path.join(data_dir, 'val')\n",
    "TRAIN_SAMPLES = 500\n",
    "VALIDATION_SAMPLES = 500\n",
    "NUM_CLASSES = 2\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and augment the data\n",
    "\n",
    "Colored images usually have 3 channels viz. red, green and blue, each with intensity value ranging from 0 to 255. To normalize it (i.e. bring the value between 0 to 1), we can rescale the image by dividing each pixel by 255. Or, we can use the default `preprocess_input` function in Keras which does the preprocessing for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   zoom_range=0.2)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to load the data from its directories and let the augmentation happen! \n",
    "\n",
    "A few key things to note:\n",
    "\n",
    "- Training one image at a time can be pretty inefficient, so we can batch them into groups. \n",
    "- To introduce more randomness during the training process, we’ll keep shuffling the images in each batch.\n",
    "- To bring reproducibility during multiple runs of the same program, we’ll give the random number generator a seed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 2 classes.\n",
      "Found 500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(TRAIN_DATA_DIR,\n",
    "                                                    target_size=(IMG_WIDTH,\n",
    "                                                                 IMG_HEIGHT),\n",
    "                                                    batch_size=BATCH_SIZE,\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=12345,\n",
    "                                                    class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    VALIDATION_DATA_DIR,\n",
    "    target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is taken care of, we come to the most crucial component of our training process - the model. We will reuse a CNN previously trained on the ImageNet dataset, remove the ImageNet specific classifier in the last few layers, and replace it with our own classifier suited to our problem. For transfer learning, we’ll ‘freeze’ the weights of the original model, i.e. set those layers as unmodifiable, so only the layers of the new classifier (that we add) can be modified. To keep things fast, we’ll choose the MobileNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker():\n",
    "    base_model = MobileNet(include_top=False,\n",
    "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False\n",
    "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    custom_model = base_model(input)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    custom_model = Dense(64, activation='relu')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
    "    return Model(inputs=input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test\n",
    "\n",
    "With both the data and model ready, all we have left to do is train the model. This is also known as fitting the model to the data. For training any model, we need to pick a loss function, an optimizer, initial learning rate and a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Валерия\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Валерия\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Валерия\\AppData\\Local\\Temp\\ipykernel_12208\\2507997108.py:5: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\Валерия\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Валерия\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8/8 [==============================] - 38s 4s/step - loss: 0.9497 - acc: 0.6140 - val_loss: 0.1834 - val_acc: 0.9420\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.3160 - acc: 0.8720 - val_loss: 0.1116 - val_acc: 0.9640\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.2070 - acc: 0.9300 - val_loss: 0.0872 - val_acc: 0.9640\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.1605 - acc: 0.9380 - val_loss: 0.0749 - val_acc: 0.9720\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1261 - acc: 0.9540 - val_loss: 0.0677 - val_acc: 0.9740\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1308 - acc: 0.9560 - val_loss: 0.0648 - val_acc: 0.9740\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0978 - acc: 0.9620 - val_loss: 0.0625 - val_acc: 0.9720\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1238 - acc: 0.9620 - val_loss: 0.0599 - val_acc: 0.9760\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.1168 - acc: 0.9620 - val_loss: 0.0646 - val_acc: 0.9720\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.0895 - acc: 0.9700 - val_loss: 0.0638 - val_acc: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x230d4c10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_maker()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['acc'])\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=math.ceil(float(TRAIN_SAMPLES) / BATCH_SIZE),\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=math.ceil(float(VALIDATION_SAMPLES) / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the 10th step, we observe about 97% validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Валерия\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prediction\n",
    "\n",
    "We can now load this model anytime and classify an image. The Keras function `load_model`, as the name suggests loads the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s try loading our original sample images and see what results we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 638ms/step\n",
      "[[7.4766518e-04 9.9925226e-01]]\n",
      "{'cat': 0, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "img_path = os.path.join(data_dir, 'val', 'dog', 'dog.693.jpg')\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "img_array = image.img_to_array(img)\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "preprocessed_img = expanded_img_array / 255.  # Preprocess the image\n",
    "prediction = model.predict(preprocessed_img)\n",
    "print(prediction)\n",
    "print(validation_generator.class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
